{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thxIeKTsQ92I"
      },
      "source": [
        "\n",
        "#**SC3000 Lab Assignment 1**\n",
        "## Group Members :\n",
        "\n",
        "\n",
        "1.   Kuan Hong Hui Joy U2221261D\n",
        "2.   Tan Pin Yee U2221027H\n",
        "3.   Cel XXX Matrix number\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLWrelR6M1Yi"
      },
      "source": [
        "#**Setting up the enviroment**\n",
        "1. Installing Neccessary Library\n",
        "2. Importing Neccessary Library\n",
        "3. Set up a virtual enviroment (not done yet)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B9jwCYxGSzz5"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import random\n",
        "\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data=''''''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCsLSTRCEGeK"
      },
      "source": [
        "##**Tutorial: Loading the CartPole environment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnMFmlrmSz25",
        "outputId": "7cac2e25-922b-4854-eaab-05f0afeef9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# Creating the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz06gsSgKGdD",
        "outputId": "32a7b538-0453-4e24-b30c-4b0cba38c66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(2)\n",
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
          ]
        }
      ],
      "source": [
        "#prints the action space of the environment\n",
        "print(env.action_space)\n",
        "\n",
        "# printing the min and max values of the 4 observed values\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOTSI64tKL0Z",
        "outputId": "5b8cbbb9-a8a1-4ff8-fe72-74825a7e2fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observations: [-0.03191902  0.03472922 -0.01207161  0.02816264]\n"
          ]
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "print(\"Initial observations:\", observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODKzoP0HO78b",
        "outputId": "3a111d49-facd-4b5b-954b-89fbbd9ea879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New observations after choosing action 0: [-0.03122443 -0.16021755 -0.01150836  0.31701252]\n",
            "Reward for this step: 1.0\n",
            "Is this round done? False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# 1st step taken in the environment\n",
        "observation, reward, done, info = env.step(0)\n",
        "print(\"New observations after choosing action 0:\", observation)\n",
        "print(\"Reward for this step:\", reward)\n",
        "print(\"Is this round done?\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYru0bTpO7u9",
        "outputId": "4c60069f-2b2b-42e4-b65f-13fd8dd14509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative reward for this round: 8.0\n"
          ]
        }
      ],
      "source": [
        "# Cumulative reward from using naive strategy\n",
        "observation = env.reset()\n",
        "cumulative_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "    observation, reward, done, info = env.step(0)\n",
        "    cumulative_reward += reward\n",
        "print(\"Cumulative reward for this round:\", cumulative_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmPipfcUc-Sz"
      },
      "source": [
        "# **Task 1: Development of an RL agent**\n",
        "\n",
        "\n",
        "*   Test out the different RL\n",
        "*   Training the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl-Q9YHMHIBr"
      },
      "source": [
        "## **Method 1 : Q learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a0uksY7EG4ZU"
      },
      "outputs": [],
      "source": [
        "#Define parameters\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 6000 #\n",
        "EPS = 1 # Epsilon\n",
        "EPS_DEC = 0.99995 #Epsilon decay value\n",
        "\n",
        "#define observation space discretization\n",
        "Observation = [30, 30, 50, 50]\n",
        "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qwbmYstoG6ii"
      },
      "outputs": [],
      "source": [
        "#Initialize Q-table with random values\n",
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "\n",
        "#Funciton to get the discrete state from the continuous state\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = state / np_array_win_size + np.array([15, 10, 1, 10])\n",
        "    return tuple(discrete_state.astype(int))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EtFGnXYOG4mC"
      },
      "outputs": [],
      "source": [
        "# Training RL Agent\n",
        "for episode in range(EPISODES + 1):\n",
        "\n",
        "    # Reset environment\n",
        "    currentState = env.reset()\n",
        "\n",
        "    discrete_state = get_discrete_state(currentState)\n",
        "    done = False\n",
        "    ep_reward = 0\n",
        "\n",
        "    #Epsilon-greedy policy\n",
        "    while not done:\n",
        "        if np.random.random() > EPS:\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        # Step action\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        #Updating Q-table\n",
        "        if not done:\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    #Decay epsilon\n",
        "    if EPS > 0.05:\n",
        "        EPS = max(EPS * EPS_DEC, 0.05)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n8upRXHdc-u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7990fe-76d7-4a3a-8a93-c7ed5ce95cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation:  [ 0.04088416  0.0064101  -0.04834092 -0.04500221]\n",
            "Action:  0\n"
          ]
        }
      ],
      "source": [
        "currentState = env.reset()\n",
        "if np.random.random() > EPS:\n",
        "    action = np.argmax(q_table[discrete_state])\n",
        "else:\n",
        "    action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "print(\"Observation: \", currentState)\n",
        "print(\"Action: \", action)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gsfy93freDQ7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGlkHoKIeDke"
      },
      "source": [
        "## **Method 2 : DQN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCM70saBix5y"
      },
      "source": [
        "\n",
        "https://github.com/VBot2410/Deep-Q-Learning-Cartpole/blob/master/Cartpole%20Deep%20Q-Learning.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "b21Q1GUUeIBT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "E-UDlFZKOPlB"
      },
      "outputs": [],
      "source": [
        "ACTIONS_DIM = 2  ## Output Dimension=No. of possible actions (2)\n",
        "OBSERVATIONS_DIM = 4  ## Input Dimension= No of Elements in State Tuple (4)\n",
        "MAX_ITERATIONS = 500  ## Max Time Steps Per Game (Limited to 500 by Environment)\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "GAMMA = 0.99  ## Future Reward Discount Factor\n",
        "REPLAY_MEMORY_SIZE = 4000  ## Replay Memory Size\n",
        "NUM_EPISODES = 500  ## Games Played in Training Phase\n",
        "MINIBATCH_SIZE = 64  ## Number of Samples chosen randomly from Replay Memory\n",
        "\n",
        "RANDOM_ACTION_DECAY = 0.99  ## The factor by which Random Action Probability Decreases\n",
        "INITIAL_RANDOM_ACTION = 1  ## Initial Random Action Factor\n",
        "Samples=[]  ## A list to store Individual Game Scores\n",
        "Means=[]  ## A list to store Mean Score over Last 20 Games\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KrKHYfSeeID5"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size):\n",
        "        self.max_size = max_size\n",
        "        self.transitions = deque()\n",
        "\n",
        "    def add(self, observation, action, reward, observation2):\n",
        "        if len(self.transitions) > self.max_size:\n",
        "            if np.random.random() < 0.5:\n",
        "                shuffle(self.transitions)\n",
        "            self.transitions.popleft()\n",
        "        self.transitions.append((observation, action, reward, observation2))\n",
        "\n",
        "    def sample(self, count):\n",
        "        return random.sample(self.transitions, count)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sbCAIKfAfoLs"
      },
      "outputs": [],
      "source": [
        "def get_q(model, observation):  ## Function to Predict Q-Values from the Model\n",
        "    np_obs = np.reshape(observation, [-1, OBSERVATIONS_DIM])  ## Reshape the state\n",
        "    return model.predict(np_obs,verbose=0)  ## Query the Model for possible actions and corresponding Q-Values\n",
        "\n",
        "def train(model, observations, targets):  ## Function to Train the Model\n",
        "    np_obs = np.reshape(observations, [-1, OBSERVATIONS_DIM])  ## Reshape the State\n",
        "    np_targets = np.reshape(targets, [-1, ACTIONS_DIM])  ## Reshape the Target\n",
        "\n",
        "    model.fit(np_obs, np_targets, epochs=1, verbose=0)  ## Fit the model using State-Target Pairs\n",
        "\n",
        "def predict(model, observation):  ## Function to Predict Q-Values from Model\n",
        "    np_obs = np.reshape(observation, [-1, OBSERVATIONS_DIM])  ## Reshape the State\n",
        "    return model.predict(np_obs,verbose=0)  ## Query the Model for possible actions and corresponding Q-Values\n",
        "\n",
        "\n",
        "\n",
        "def get_model():  ## Deep Q-Network\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(24, input_shape=(OBSERVATIONS_DIM, ), activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(2, activation='linear'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(lr=LEARNING_RATE),  ## Adam Optimizer with Initial Learning Rate=0.001\n",
        "        loss='mse',\n",
        "        metrics=[],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def update_action(action_model, target_model, sample_transitions):\n",
        "    random.shuffle(sample_transitions)  ## Randomly Shuffle the Minibatch Samples\n",
        "    batch_observations = []\n",
        "    batch_targets = []\n",
        "\n",
        "    for sample_transition in sample_transitions:  ## For each sample in Minibatch\n",
        "        old_observation, action, reward, observation = sample_transition\n",
        "\n",
        "        targets = np.reshape(get_q(action_model, old_observation), ACTIONS_DIM)\n",
        "        targets[action] = reward  ## Set Target Value\n",
        "        if observation is not None:  ## If observation is not Empty\n",
        "\n",
        "            ## Query the Model for possible actions and corresponding Q-Values\n",
        "            predictions = predict(target_model, observation)\n",
        "            new_action = np.argmax(predictions)  ## Select the Best Action (Max Q-Value)\n",
        "\n",
        "            ## Update the Target with Future Reward Discount Factor\n",
        "            targets[action] += GAMMA * predictions[0, new_action]\n",
        "\n",
        "        batch_observations.append(old_observation)  ## Add Old State to observations batch\n",
        "        batch_targets.append(targets)  ## Add target to targets batch\n",
        "\n",
        "    ## Update the model using Observations and their corresponding Targets\n",
        "    train(action_model, batch_observations, batch_targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C6vvacvlf3bU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98bfMmpMRNAg",
        "outputId": "59cad993-58da-41cd-eeb0-bc16afe513b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:0, iterations:23, RAP:0.99\n",
            "Episode:1, iterations:24, RAP:0.9801\n",
            "Episode:2, iterations:23, RAP:0.9702989999999999\n",
            "Episode:3, iterations:19, RAP:0.96059601\n"
          ]
        }
      ],
      "source": [
        "## contains the training and testing results, so maybe can split the testing phase into task 2\n",
        "##also the training takes damn long, maybe close to an hr\n",
        "def main():\n",
        "    Temp=[]  ## to hold most recent 100 Game Scores\n",
        "    iteration=0  ## Initialize Time Step Number to Avoid initial No variable Error\n",
        "    ## Set Random Action Probability to Initial Number(=1)\n",
        "    random_action_probability = INITIAL_RANDOM_ACTION\n",
        "\n",
        "    replay = ReplayBuffer(REPLAY_MEMORY_SIZE)  ## Initialize Replay Memory & Specify Maximum Capacity\n",
        "\n",
        "    action_model = get_model()  ## Initialize action-value model with random weights\n",
        "\n",
        "    env = gym.make('CartPole-v1')  ## Prepare the OpenAI Cartpole-v1 Environment\n",
        "\n",
        "    for episode in range(NUM_EPISODES):  ## For Games 0 to Maximum Games Limit\n",
        "        if np.mean(Temp) > 495 and iteration > 495: ## Training the model until it passes\n",
        "            print('Passed')\n",
        "            break\n",
        "\n",
        "        ## If mean over the last 100 Games is >495, then Success!!!\n",
        "        if np.mean(Temp)>495 and iteration>495:\n",
        "            print('Passed')\n",
        "            break\n",
        "\n",
        "        ## Reduce the Random Action Probability by Decay Factor\n",
        "        random_action_probability *= RANDOM_ACTION_DECAY\n",
        "        observation = env.reset()\n",
        "\n",
        "        for iteration in range(MAX_ITERATIONS):  ## Timesteps\n",
        "            ## Generate Random Action Probability\n",
        "            random_action_probability = max(random_action_probability, 0.1)\n",
        "            old_observation = observation\n",
        "            ## If generated fraction<Random Action Probability\n",
        "            if np.random.random() < random_action_probability:\n",
        "                ## Take Random Action (Explore)\n",
        "                action = np.random.choice(range(ACTIONS_DIM))\n",
        "            else:  ## If generated fraction>Random Action Probability\n",
        "                ## Query the Model and Get Q-Values for possible actions\n",
        "                q_values = get_q(action_model, observation)\n",
        "                action = np.argmax(q_values)  ## Select the Best Action using Q-Values received\n",
        "            ## Take the Selected Action and Observe Next State\n",
        "            observation, reward, done, info = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                Samples.append(iteration+1)  ## Add Final Score of the Game to the Scores List\n",
        "                ## If Number of Games>100, Calculate Mean Over 100 Games to Check Convergence\n",
        "                if len(Samples)>100:\n",
        "                    Temp=Samples[-100:]  ## Select Score of Most Recent 100 Games\n",
        "                    ## Add Mean of Most Recent 20 Games to a list\n",
        "                    Means.append(np.mean(Samples[-20:]))\n",
        "                ## Print End-Of-Game Information\n",
        "                print(('Episode:{}, iterations:{}, RAP:{}').format(\n",
        "                        episode,\n",
        "                        iteration,random_action_probability))\n",
        "\n",
        "                if iteration!=499:\n",
        "                    reward = -5  ## Give -5 Reward for Taking Wrong Action Leading to Failure\n",
        "                if iteration==499:\n",
        "                    reward= 5  ## Give +5 Reward for Completing the Game Successfully\n",
        "                ## Add the Observation to Replay Memory\n",
        "                replay.add(old_observation, action, reward, None)\n",
        "                break  ## Break and Start a new Game\n",
        "\n",
        "            ## Add the Observation to Replay Memory\n",
        "            replay.add(old_observation, action, reward, observation)\n",
        "\n",
        "            ##  Update the Deep Q-Network Model\n",
        "            if replay.size() >= MINIBATCH_SIZE and np.random.random()<0.25 and Samples[-1]<495:\n",
        "                sample_transitions = replay.sample(MINIBATCH_SIZE)\n",
        "                update_action(action_model, action_model, sample_transitions)\n",
        "\n",
        "\n",
        "####\n",
        "##  Here, the Training Phase Ends. Now We plot the Training Results and Save the Trained Model\n",
        "##  We Also Test the Model for Another 100 Games.\n",
        "####\n",
        "\n",
        "    print(\"Training Process\")\n",
        "    plt.plot(Samples)\n",
        "    plt.title('Training Phase')\n",
        "    plt.ylabel('Cumulative Rewards')\n",
        "    plt.ylim(ymax=510)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.savefig('Training.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## can remove this graph\n",
        "    plt.plot(Means)\n",
        "    plt.title('Mean Score of Last 20 Games')\n",
        "    plt.ylabel('Time Steps')\n",
        "    plt.ylim(ymax=510)\n",
        "    plt.xlabel('Trial')\n",
        "    plt.savefig('Training_Average.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "    action_model.save('cartpole_model.h5')  ## Save the Trained Model\n",
        "    del action_model\n",
        "    action_model = load_model('cartpole_model.h5')  ## Load the Trained Model\n",
        "\n",
        "\n",
        "## Here We test the trained model for 100 more games\n",
        "    Tests=[]\n",
        "    for i in range(0,100):\n",
        "        observation = env.reset()\n",
        "        while(True):\n",
        "            old_observation = observation\n",
        "            q_values = get_q(action_model, observation)\n",
        "            action = np.argmax(q_values)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "                Tests.append(iteration+1)\n",
        "                env.reset()\n",
        "                break\n",
        "    print(np.mean(Tests))\n",
        "    plt.plot(Tests)\n",
        "    plt.title('Testing Phase')\n",
        "    plt.ylabel('Time Steps')\n",
        "    plt.ylim(ymax=510)\n",
        "    plt.xlabel('Trial')\n",
        "    plt.savefig('Testing.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7LfBJTegcwf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL-7yCGBGp-4"
      },
      "source": [
        "#**Task 2: Demonstrate the effectiveness of the RL Agent**\n",
        "*   Test the trained models against 100 episodes\n",
        "*   Plot the graph for each method\n",
        "*   Make comparision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq6wLpdOPjIX"
      },
      "source": [
        "## **Method 1 : Q-learning**\n",
        "Running the trained Q-learning agent with 100 episode and plotting its cumulative rewards onto a graph to see the effectiveness of the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHschWrSS932"
      },
      "outputs": [],
      "source": [
        "# Testing the agent\n",
        "num_episodes = 100\n",
        "max_time_steps = 500\n",
        "rewards = []\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    discrete_state = get_discrete_state(state)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        action = np.argmax(q_table[discrete_state])\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        discrete_state = get_discrete_state(state)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    #Printing results of each episode\n",
        "    rewards.append(total_reward)\n",
        "    print('Episode {}: Cumulative Reward = {}'.format(i+1, total_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbEdk0NAS96B"
      },
      "outputs": [],
      "source": [
        "# Plot cumulative rewards\n",
        "plt.plot(rewards)\n",
        "plt.title('Cumulative reward for each episode')\n",
        "plt.ylabel('Cumulative reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.show()\n",
        "\n",
        "# Calculate average cumulative reward\n",
        "average_reward = sum(rewards) / num_episodes\n",
        "print(\"Average cumulative reward:\", average_reward)\n",
        "print(\"Is my agent good enough?\", average_reward > 195)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2OaRFABS98W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4c-b8TFdyQL"
      },
      "source": [
        "## **Method 2 : DQN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibC3xF9Rd2Ce"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHHizVH0d2Ee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLUGK0Fp7k9a"
      },
      "source": [
        "# Task 3: Render one episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Cad6e41ZMrE"
      },
      "outputs": [],
      "source": [
        "\n",
        "env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
        "observation = env.reset()\n",
        "while True:\n",
        "    env.render()\n",
        "    #your agent goes here\n",
        "    action = rand_policy_agent(observation)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "\n",
        "    if done:\n",
        "      break;\n",
        "env.close()\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}